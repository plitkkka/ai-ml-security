**AI and ML Security**

Создание классификатора на основе сверточной нейронной сети для распознавания изображений из набора данных Fashion MNIST. Изначально, на тестовом наборе, модель достигла точности 90.32%.
В процессе работы был применён метод искажения FGSM (Fast Gradient Sign Method), чтобы создать состязательные примеры, которые могут обмануть нашу модель. Точность модели на этих искажённых данных снизилась до 71.65%, что продемонстрировало уязвимость 
классификатора к подобным атакам. 
Далее, были применены защитные механизмы, такие как состязательное обучение и защитная дистилляция. Состязательное обучение включало обучение модели на искажённых примерах, что позволило ей лучше адаптироваться к атакам и повысить устойчивость. 
Защитная дистилляция, в свою очередь, использовала «мягкие метки» от модели-учителя для обучения модели-дистиллятора, что способствовало улучшению её общей производительности.
Результаты защиты оказались эффективными: точность защищённого классификатора на состязательных образцах возросла до 92.00%, а метрики качества, такие как Precision, Recall и F1-score, также показали значительные улучшения. 
Эти результаты свидетельствуют о том, что внедрение защитных механизмов сделало модель более надежной и устойчивой к искажениям.

> Описание исходной модели классификатора и зафиксированных исходных значений метрик качества.
Исходная модель представляет собой сверточную нейронную сеть (CNN), предназначенную для классификации изображений из набора данных Fashion MNIST, который содержит 10 классов одежды и аксессуаров. 
Модель имеет следующую архитектуру:
•	Входной слой: Размер входного изображения: 28x28 пикселей с одним каналом (оттенки серого).
•	Сверточные слои (Conv2D):
1.	Первый сверточный слой: 32 фильтра с размером ядра  3x3, функция активации — ReLU.
2.	Первый слой подвыборки (MaxPooling): Максимальное объединение с размером окна 2x2.
3.	Второй сверточный слой: 64 фильтра с размером ядра 3x3, функция активации — ReLU.
4.	Второй слой подвыборки (MaxPooling): Максимальное объединение с размером окна 2x2.
5.	Третий сверточный слой: 64 фильтра с размером ядра 3x3, функция активации — ReLU.
•	Полносвязные слои (Dense):
1.	Слой выравнивания (Flatten): Преобразует 2D-данные из сверточных слоев в одномерный вектор.
2.	Полносвязный слой: 64 нейрона, функция активации — ReLU.
3.	Выходной слой: 10 нейронов (по числу классов), функция активации — линейная.
•	Оптимизатор: Adam (Adaptive Moment Estimation).

После обучения на 5 эпохах с размером пакета данных(батча) 32, модель достигла следующих результатов на тестовых данных:
•	Accuracy: 0.9032 (Это означает, что модель правильно классифицировала 90.32% тестовых изображений.)
•	Precision: 0.9037 
•	Recall: 0.9032
•	F1-score: 0.9023

> Описание механизма реализации угрозы искажения, направленной на нарушение работы и снижение достоверности классификатора.

Одним из самых известных методов создания искажающих примеров является FGSM (Fast Gradient Sign Method). FGSM вносит искажения в исходное изображение, добавляя или вычитая небольшие значения из каждого пикселя изображения на основе знака градиента функции потерь.
Были использованы следующие шаги реализации FGSM:
1.	Вычисление функции потерь для текущих данных: модель сначала предсказывает результаты для данных, и на основе этих предсказаний вычисляется ошибка с помощью функции потерь.
2.	Вычисление градиента потерь по входным данным: используется метод обратного распространения ошибки для вычисления градиента функции потерь по отношению к входным данным.
3.	Создание искажающих примеров: на основе вычисленного градиента к входным данным добавляется искажение с малым коэффициентом, направленное на увеличение ошибки модели.
Параметр epsilon контролирует величину искажения.

> Описание примеров полученных состязательных образцов, подаваемых на вход классификатора.
При помощи метода FGSM (Fast Gradient Sign Method) добавляются минимальные искажения к каждому пикселю изображения. Эти изменения направлены на увеличение функции потерь модели, что ухудшает её способность правильно классифицировать искажённое изображение.
Пример полученных состязательных объектов: допустим, у нас есть исходное изображение кроссовка. После применения FGSM к этому изображению, модель может "увидеть" искажения, которые для человека остаются незаметными, но для модели изменяют ключевые характеристики объекта. Это может привести к тому, что модель ошибочно распознает кроссовок как сандалию или рубашку.
Генерация искажающих примеров была реализована следующим образом:
•	adversarial_test_images = test_images + create_adversarial_pattern(model, test_images, test_labels) * 0.1
•	adversarial_test_images = tf.clip_by_value(adversarial_test_images, 0, 1)

> Представление значений метрик качества, полученных для исходного классификатора при воздействии состязательных образцов.
Для оценки устойчивости модели к атаке, были измерены стандартные метрики качества на искажённых данных. Полученные результаты:
•	Accuracy: 0.7165;
•	Precision: 0.7340;
•	Recall: 0.7165;
•	F1-score: 0.7202;
Эти результаты демонстрируют, что проведенная атака прошла успешно и значительно снижает качество классификации исходного классификатора. Все метрики ухудшились по сравнению с тестированием на исходных данных.

> Описание построенных защитных механизмов.
1.	Защита на основе состязательного обучения
Состязательное обучение представляет собой метод, в котором модель обучается на специально сгенерированных состязательных примерах, чтобы повысить свою устойчивость к атакам.
•	Используя метод FGSM, для исходных изображений из обучающей выборки генерируются состязательные образцы. Этот метод изменяет исходные изображения, добавляя небольшие искажения в направлении градиента потерь.
•	Модель обучается не только на обычных примерах, но и на сгенерированных состязательных образцах, что позволяет ей научиться правильно классифицировать данные даже в случае атаки.
2.	Защитная дистилляция
Защитная дистилляция — это метод, который используется для обучения более устойчивой модели, называемой моделью-дистиллятором, путем обучения на "мягких метках", которые содержат более полную информацию о распределении классов.
Основные шаги:
•	Вместо жестких классов, на которые обучается обычная модель, дистилляционная модель обучается на мягких метках, которые содержат вероятности, предсказанные исходной моделью. Это позволяет модели лучше улавливать малозаметные различия между классами.
Благодаря использованию мягких меток и агрессивному обучению на состязательных примерах, модель-дистиллятор становится менее восприимчивой к атакам, так как она лучше улавливает мелкие изменения в данных и игнорирует малозначимые искажения.
Эти механизмы работают в связке для минимизации потерь в точности модели при атаке, помогая классификатору оставаться более надёжным в условиях искажения данных.

> Представление значений метрик качества, полученных для защищенного классификатора при воздействии состязательных образцов.
Метрики для защищённого классификатора при воздействии состязательных образцов:
•	Accuracy: 0.9200; (Модель корректно классифицирует 92% примеров)
•	Precision: 0.8778;
•	Recall: 0.8730;
•	F1-score: 0.8746;
Эти метрики демонстрируют значительное повышение устойчивости модели по сравнению с метриками классификатора без защиты (точность без защиты составляла 0.7165). Применение состязательного обучения и защитной дистилляции позволило модели сохранить высокую точность.

